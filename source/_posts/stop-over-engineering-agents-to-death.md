---
title: 停止用工程化扼杀 Agent
date: 2026-01-23 13:51:00
updated: 2026-01-23 13:51:00
tags:
  - AI
  - Agent
  - LLM
  - AGI
categories:
  - [gallery]
featured_image: /gallery/ai-evolution/cover.png
author: chenli & gpt5.2 & claude 4.5
---

# 停止用工程化扼杀 Agent

## ——我们需要赋能，而非控制

过去一年，Agent 的演进路径高度一致：通过工程化手段提升稳定性与可控性。

更复杂的 prompt，更精细的 workflow，更多 planner、critic、memory、tool、guardrail。

这些手段让 Agent 更容易进入生产环境，也更容易被评估和部署。但我们需要直面一个现实：**这种路线在提高可控性的同时，正在系统性地扼杀 Agent 的核心能力。**

---

## 一、工程化手段正在扼杀什么

当前主流 Agent 架构的核心假设是：智能体可以被拆解为一组可组合的功能模块。

规划、执行、反思、工具调用、状态管理，各自独立设计，再通过编排连接。这种模块化思想在传统软件工程中行之有效，但应用到 Agent 上时，产生了三个致命的副作用：

### 扼杀模式 1：用预设流程替代决策能力

Agent 不再"判断"下一步该做什么，而是"执行"工程师预设的流程。决策权被剥夺，推理能力退化为流程遍历。

### 扼杀模式 2：用固定路径压缩探索空间

可能的行动路径被工程师的经验框定。Agent 只能在预设的有限选项中选择，无法发现新的解决方案，更无法在环境变化时调整策略。

### 扼杀模式 3：用回退机制剥夺学习机会

遇到边界情况时，系统设计为回退到默认流程或等待人工介入。每次失败本应是宝贵的学习信号，却被"保护机制"浪费。Agent 永远无法从经验中成长。

**我们用工程纪律换取了稳定性，却扼杀了智能涌现的可能。** Agent 变得更可控，也变成了一个精密的自动机——它可以高效执行，却无法真正思考。

### 典型案例：被工程化扼杀的客服 Agent

某电商公司的客服 Agent 被精心设计为：

- 意图识别模块：17 种预定义意图
- 槽位填充模块：每个意图 3-8 个必填字段
- 规则引擎：142 条 if-then 规则
- 回复生成模块：基于模板 + 少量生成

这套系统看起来完备，实际上已经把 Agent 变成了一个复杂的决策树遍历器。

**场景：**用户说"我那个东西还没到，能退吗？"

**系统机械执行：**
识别意图（退款）→ 检测必填槽位（订单号）→ 判断缺失 → 触发槽位填充流程 → 输出："请提供您的订单号"

**用户真实需求：**
商品延迟配送，担心无法按时收到，想知道能否直接取消订单并退款。用户的焦虑在于时间，而不是订单号。

**一个具备认知能力的 Agent 应该：**
1. 理解用户的情绪（焦虑）和真实目标（取消延迟订单）
2. 判断是否真的需要订单号（可以通过账号查询近期订单）
3. 先安抚用户，说明可以处理，再获取必要信息
4. 如果用户无法提供订单号，主动提供替代方案

**17 种预定义意图成为了认知的天花板，142 条规则成为了智能的囚笼。**工程化设计让这个 Agent 在标准场景下运行良好，却扼杀了它理解用户真实意图的能力。

---

## 二、Agent 的核心价值不是执行，而是认知

一个真正有价值的 Agent，不取决于有多少个模块，而取决于是否存在一个持续运行的**认知闭环**：

**理解环境 → 形成判断 → 采取行动 → 评估反馈 → 更新认知**

这个闭环要求三种核心能力：

- **对环境的长期建模能力**：不只是记住最近几轮对话，而是持续构建对任务、用户、历史的理解
- **对目标的持续保持能力**：在执行过程中始终明确"我在做什么"、"为什么这样做"
- **对自身行为的评估能力**：能够判断"这个行动是否有效"、"是否需要调整策略"

**当工程结构开始替代这些能力，Agent 就从一个可以成长的认知体退化为一个被驱动的执行器。**

### 对比：游戏 AI 的两种设计哲学

**流程型设计（被工程化扼杀）：**

一个 RTS 游戏 AI 被设计为状态机：
- 阶段 1：侦察（固定建造 2 个侦察单位）
- 阶段 2：建造（按预设序列建造建筑）
- 阶段 3：扩张（达到资源阈值后扩张）
- 阶段 4：进攻（攒够 20 个单位后进攻）

**结果：**这种 AI 在中等难度下表现稳定，但无法应对非常规战术。遇到 Rush 战术时，它仍然按部就班地执行"侦察-建造-扩张"流程，直到被碾压。**行为空间已被流程锁死。**

**认知闭环设计（赋能型）：**

AlphaStar 在《星际争霸 2》中的设计：
- 持续维持对整个游戏状态的理解：资源分布、敌方单位、科技树进度、地图控制
- 没有预设的"开局套路"，而是在每个时刻基于当前状态选择最优行动
- 遇到陌生战术时，实时评估威胁并调整策略
- 失败后，更新对特定战术的评估模型，而非简单增加 if-then 规则

**核心差异：**前者执行预定义策略，后者在环境中形成策略。前者是自动机，后者是认知体。

---

## 三、现实证据：Claude Code 如何通过认知闭环释放智能

Claude Code 提供了一个清晰的对照实验。

从工程角度看，它的架构异常简单：一个大模型 + 对代码仓库、终端、文件系统的直接访问。它没有复杂的多 Agent 协作，没有厚重的流程控制层，没有精心设计的状态机。

但在真实软件工程任务中，它的表现显著优于大量流程化 Agent。**关键差异不在工程复杂度，而在认知闭环的完整性。**

### 能力 1：持续构建的世界模型

Claude Code 在工作过程中逐步构建对代码库的内部表示：
- 项目架构：哪些模块，如何组织
- 依赖关系：模块之间的调用关系
- 构建状态：当前通过了哪些测试，失败了哪些
- 历史修改：之前改过什么，为什么改

这些信息**不是被压缩成短期上下文，而是持续影响后续判断**。它可以跨文件、跨步骤推理问题。

**实例：**用户说"登录功能有问题"

**流程化 Agent 的做法：**
1. 步骤 1：搜索关键词 "login"
2. 步骤 2：返回包含该关键词的文件列表
3. 步骤 3：等待用户指定查看哪个文件

**Claude Code 的做法：**
1. 回忆之前对认证流程的理解（JWT 还是 Session？在哪个模块？）
2. 推断可能涉及的组件（auth service、user model、middleware、路由）
3. 检查最近相关的代码修改（git log）
4. 运行测试，定位具体失败点
5. 基于失败信息，判断是认证逻辑问题还是配置问题

差异在于：前者在"搜索"，后者在"推理"。前者依赖关键词匹配，后者基于世界模型理解问题。

### 能力 2：稳定的高层目标

流程化 Agent 的常见问题：任务被拆成离散步骤，由系统调度执行。Agent 只知道"当前步骤是什么"，不知道"整体目标是什么"。

Claude Code 维持一个清晰的高层目标，例如"修复这个 bug"、"让所有测试通过"、"实现这个 feature"。在这个目标指引下，它**自由选择路径**：查看日志、修改代码、调整配置、运行测试。

**对比：**

**流程化 Agent：**
"步骤 1：分析需求 → 步骤 2：查找相关文件 → 步骤 3：修改代码 → 步骤 4：运行测试"
*步骤固定，顺序固定，无法根据实际情况调整*

**Claude Code：**
"目标：让测试通过。当前观察：3 个测试失败，都与日期格式有关。判断：可能是时区配置问题。决策：先检查配置文件，如果不是配置问题再看代码。"
*目标明确，路径灵活，基于实际情况决策*

### 能力 3：来自真实环境的即时反馈

Claude Code 的每个行动都直接作用于真实系统，并获得即时反馈：
- 修改代码 → 编译错误/成功
- 运行测试 → 通过/失败 + 详细错误信息
- 执行命令 → 输出结果

这些反馈进入下一轮推理，形成闭环：**行动 → 反馈 → 更新世界模型 → 新决策**

**认知闭环的完整演示：**
```
[行动] 修改 API 参数验证逻辑
[反馈] 编译通过，但 3 个集成测试失败
[更新世界模型] 发现下游服务依赖旧的参数格式
[新的理解] 这不是简单的 bug 修复，而是接口变更
[决策] 需要兼容方案：新旧参数格式都支持
[行动] 添加参数转换中间件
[反馈] 所有测试通过
[巩固认知] 记住"修改 API 参数需要考虑向后兼容"
```

### 能力 4：拥有自由度

Claude Code 最重要的特征：**它可以自主决定看什么文件、运行什么命令、下一步做什么。**

这种自由度让模型的推理能力完整释放。流程化 Agent 通过控制路径换取稳定性，Claude Code 通过赋予自由度释放智能。

**这不是工程上的偷懒，而是对 Agent 本质的深刻理解：智能不是执行能力，而是选择能力。**

---

## 四、停止扼杀，用“赋能”取代“工程”

问题不在于工程化本身，而在于我们用工程化解决了什么问题。

大多数工程化优化的目标是：**降低不确定性**。流程、规则、校验器的作用是压缩行为空间，把"智能决策"替换为"确定执行"。

**这正在扼杀 Agent 最宝贵的能力：在不确定环境中探索、学习、适应。**

真正的工程化应该是：**为 Agent 的能力增长搭建基础设施**，而不是用流程和规则限制它的行为空间。

### 原则 1：用环境反馈替代预定义规则

**停止扼杀智能的做法：**
```python
def handle_user_query(query):
    if "退款" in query:
        return refund_flow()
    elif "发货" in query:
        return shipping_flow()
    # ... 300 个 if-elif 分支
    # 每个分支都是对 Agent 判断力的替代
```

这种设计的本质：**工程师用自己的经验替代了 Agent 的学习能力**。300 个分支意味着 Agent 永远只能处理这 300 种情况，遇到第 301 种就会失败。

**释放智能的做法：**
```python
def handle_user_query(query, context):
    # 让 Agent 理解意图和上下文
    intent = agent.understand(query, context)
    
    # 让 Agent 自主决策行动
    action = agent.decide_action(intent)
    result = execute(action)
    
    # 用真实反馈指导学习
    if result.success:
        agent.reinforce(action, result)
        return result
    else:
        # 从失败中学习，而非回退到默认流程
        agent.learn_from_failure(intent, action, result.error)
        return agent.retry_with_new_strategy(intent, result)
```

**关键认知：**前者用规则限制行为，后者用反馈塑造能力。

### 原则 2：维持长期状态，停止用短期上下文限制记忆

**正在扼杀智能的做法：**每次调用只传入最近 3 轮对话，Agent 像失忆症患者，无法形成连续理解。

这背后的假设：**长期记忆会增加复杂度和不可控性**。但这同时剥夺了 Agent 最关键的能力——从经验中学习。

**释放智能的做法：**
- 为 Agent 提供持久化的记忆层
- 包含：历史交互模式、用户偏好演化、任务执行轨迹、失败案例分析
- 让 Agent 可以说："上次遇到类似问题，方案 A 失败了，方案 B 成功了"

**对比：代码审查 Agent**

**短期记忆（被扼杀）：**
- 每次审查都重新学习团队风格
- 重复指出已讨论过的问题
- 无法理解"这次我们想尝试不同方案"

**长期记忆（被赋能）：**
- 记住团队倾向函数式风格
- 知道上次因过度抽象产生了问题
- 理解核心模块需要更严格的测试覆盖
- 能识别"这个 PR 在尝试解决之前讨论的技术债"

**长期记忆不是负担，而是智能的基础。**

### 原则 3：赋予行动选择权，停止用流程剥夺决策

**正在扼杀智能的做法：**"步骤 1 必须完成才能进入步骤 2，步骤 3 依赖步骤 2 的输出"

这种 DAG 式任务编排看起来清晰可控，实际上在说：**我们不信任 Agent 的判断力**。工程师预设了唯一正确的执行路径，Agent 沦为流程执行器。

当环境变化（步骤 2 的假设不成立，或有更高效路径），Agent 无法调整，只能机械完成流程或报错等待人工。

**释放智能的做法：**

给 Agent 一组可用工具和清晰目标，**让它自己规划执行路径**。

**示例：数据分析 Agent**

**目标：**找出销售下降原因
**可用工具：**查询数据库、绘制图表、读取日志、调用外部 API、查看历史报告

**Agent 自主决策的路径：**
1. 先看整体销售趋势图 → 发现某地区异常下降
2. 深入该地区细分数据 → 发现是某品类问题
3. 对比竞品动态 → 发现竞品降价促销
4. 查看库存和物流 → 排除供应链问题
5. 形成假设：竞品价格战导致
6. 验证假设：调取价格敏感度历史数据

**如果是流程化设计：**预设路径可能是"先查品类 → 再查地区 → 最后看竞品"。这个顺序在当前问题下低效，但 Agent 被流程锁死，无法优化。

**智能不是执行能力，而是选择能力。**

### 原则 4：让失败成为学习信号，停止用回退机制保护 Agent

**正在扼杀智能的做法：**遇到错误立即回退到默认流程，或者触发人工接管。

这背后的逻辑是：**Agent 不可信，失败是风险**。但这种保护机制同时剥夺了 Agent 最重要的学习机会。每次失败都是环境给出的明确反馈信号，告诉 Agent"这条路不通"。

当我们用回退机制"保护" Agent 时，实际上是在说：你不需要学习，只需要执行；遇到问题，人类会救你。

**释放智能的做法：**
```python
class CognitiveAgent:
    def execute_task(self, task):
        attempts = []
        
        while not task.completed and attempts < max_attempts:
            # 基于历史尝试生成新策略
            plan = self.generate_plan(task, attempts)
            result = self.execute(plan)
            attempts.append((plan, result))
            
            if result.failed:
                # 分析失败：是假设错误？工具使用不当？环境变化？
                diagnosis = self.diagnose_failure(result)
                
                # 更新世界模型：记住这个失败模式
                self.update_world_model(diagnosis)
                
                # 调整策略：不是回退，而是进化
                self.adjust_strategy_based_on_failure(diagnosis)
                continue
            
            task.update(result)
        
        # 如果真的无法完成，寻求帮助时要带着分析
        if not task.completed:
            return self.request_help_with_context(task, attempts)
```

**真实案例：自动化测试 Agent**

运行测试时遇到环境问题：`Error: Port 8080 already in use`

**回退型处理：**报错，标记任务失败，等待人工介入。下次遇到同样问题，继续失败。

**学习型处理：**
1. 识别错误类型：端口冲突
2. 生成解决假设：可能有残留进程、可以换端口、可以清理环境
3. 尝试解决方案：`lsof -i :8080` 查看占用进程
4. 发现是上次测试的残留进程，执行 kill
5. 重新运行测试，成功
6. **关键**：将"端口冲突"模式记录到知识库，下次优先检查

三次遇到类似问题后，Agent 学会了在启动测试前主动检查端口。这就是从失败中成长。

### 原则 5：设计可观测的内部状态

**Agent 应该能够"自省"：**
- 当前理解的目标是什么
- 已知的环境状态有哪些
- 为什么选择当前行动
- 对结果的预期是什么
- 当前的不确定性在哪里

这不是为了可解释性合规，而是让 Agent 具备**元认知能力**——对自己思考过程的认知。

```python
class CognitiveAgent:
    def __init__(self):
        self.world_model = {}      # 对环境的理解
        self.goal_stack = []       # 当前目标层级
        self.hypotheses = []       # 活跃的假设
        self.uncertainty_map = {}  # 不确定性评估
    
    def introspect(self):
        return {
            "current_goal": self.goal_stack[-1],
            "world_state_summary": self.summarize_world_model(),
            "active_hypotheses": self.hypotheses,
            "confidence_level": self.estimate_confidence(),
            "known_unknowns": self.identify_gaps(),
            "next_actions_rationale": self.explain_decision()
        }
    
    def should_seek_help(self):
        # Agent 知道自己什么时候需要帮助
        intro = self.introspect()
        return (
            intro["confidence_level"] < threshold or
            len(intro["known_unknowns"]) > acceptable_uncertainty
        )
```

**关键价值：**一个能够自省的 Agent，才能够自我调整、自我改进。

### 原则 6：区分"必要约束"与"过度控制"

**关键认知：不是所有工程化都在扼杀智能。**

问题在于我们常常混淆两种不同性质的约束：

**必要约束（应该保留）：**
- **安全边界**：不允许删除生产数据库、不处理敏感信息
- **成本控制**：API 调用预算、计算资源上限
- **合规要求**：数据处理规范、隐私保护标准
- **不可逆操作确认**：删除、支付等操作需要明确确认

这些约束保护的是**系统安全**和**业务底线**，它们定义了 Agent 不应该做什么。

**过度控制（正在扼杀智能）：**
- **预设的思考步骤**："必须先 A 再 B 后 C"
- **固化的工具调用序列**："只能按这个顺序使用工具"
- **人为压缩的观察空间**："只允许看这 5 个维度的数据"
- **强制的决策树**："遇到情况 X 必须执行动作 Y"

这些约束限制的是**Agent 的认知空间**和**决策自由度**，它们规定了 Agent 应该如何思考。

**实践建议：设置"护栏"而非"轨道"**

- **护栏（Guardrails）**：定义不可逾越的边界，内部自由探索
- **轨道（Rails）**：预设路径，限制所有行动

**示例：**

```python
# 轨道式设计（过度控制）
def customer_service_flow(query):
    step1 = identify_intent(query)  # 必须先识别意图
    step2 = fill_slots(step1)       # 必须填充槽位
    step3 = query_database(step2)   # 必须查询数据库
    return generate_response(step3)  # 必须生成回复

# 护栏式设计（赋能）
def customer_service_agent(query, context):
    # 护栏：安全边界
    if would_violate_privacy(query):
        return safety_response()
    
    # 自由：Agent 自主决策如何处理
    understanding = agent.understand(query, context)
    
    # 护栏：成本控制
    with resource_limit(max_api_calls=10):
        solution = agent.solve(understanding)
    
    # 护栏：质量检查
    if not meets_quality_standard(solution):
        solution = agent.refine(solution)
    
    return solution
```

---

## 五、更多实例：认知闭环 vs 流程控制

### 案例 1：医疗诊断 Agent

**流程型设计（被扼杀）：**

采用决策树结构：询问症状 → 匹配症状库 → 计算疾病概率 → 给出建议。每个分支都有预定义的问题序列。

**问题：**当患者描述"最近总是累，晚上睡不好"，系统会按疲劳症状的标准问题序列询问："疲劳持续多久？是否伴有发热？体重有变化吗？"

但患者的核心问题可能是焦虑导致的睡眠障碍，而非生理性疲劳。系统被决策树锁定，无法调整方向。

**认知闭环设计（被赋能）：**

Agent 维持一个动态的"患者状态模型"：
- 主诉症状（疲劳、失眠）
- 可能的病因假设（感染、代谢异常、心理因素、药物副作用）
- 已排除的可能性（通过问答逐步更新）
- 需要进一步确认的方向（根据新信息动态调整）

每次对话，Agent 根据新信息更新假设空间，而非执行固定流程：
- 患者提到"工作压力大" → 立即提高心理因素权重
- 询问方向转向：睡眠模式、焦虑症状、压力源
- 如果患者提到"咖啡喝得多" → 考虑咖啡因影响
- 动态调整假设优先级

**关键差异：**前者执行决策树，后者维持假设空间并动态调整。

### 案例 2：内容审核 Agent

**流程型设计（被扼杀）：**

关键词过滤 → 分类器判断 → 人工复审队列。规则明确，但面对新型违规内容（如隐晦的诱导话术、暗语、图文混合对抗）反应滞后。

**认知闭环设计（被赋能）：**

Agent 维持对"违规模式"的持续学习模型：
1. 看到可疑内容时，不只是二分类（通过/拒绝）
2. 而是生成违规假设："这段文字可能在用暗语推广违禁品"
3. 搜索上下文信息：用户历史行为、关联账号、发布时间模式、评论互动
4. 基于证据链更新判断
5. 将边界案例反馈到模型更新流程
6. 学习新出现的对抗模式

**实例：**

某用户发布："想要那个'快乐水'，懂的私聊"

**流程型 Agent：**检查关键词"快乐水"，未在违禁词库中，通过。

**认知闭环 Agent：**
1. 识别可疑模式：暗语、"懂的"、私聊诱导
2. 查看用户历史：频繁使用类似话术
3. 查看关联账号：存在已封禁账号
4. 分析评论：有人回复"老板，暗号多少"
5. 综合判断：高概率违规
6. 标记为违规，并记录"快乐水"在此上下文中的新含义

这种设计让 Agent 能够应对违规内容的持续演化。

### 案例 3：科研文献 Agent

**流程型设计（被扼杀）：**

关键词搜索 → 摘要提取 → 相关性评分 → 排序返回

**认知闭环设计（被赋能）：**

- 理解研究问题的核心（不只是关键词）
- 构建相关领域的知识图谱
- 识别关键作者、引用网络、研究前沿
- 迭代搜索：初步结果 → 发现新方向 → 调整搜索策略
- 综合判断：这篇文献是开创性的、综述性的、还是边缘验证？

**用户问："神经网络压缩的最新进展"**

**流程型 Agent：**
搜索关键词"神经网络压缩"，返回包含这些词的论文列表，按发表时间排序。

**认知闭环 Agent：**
1. 理解"压缩"包含多个子领域：剪枝、量化、知识蒸馏、架构搜索
2. 识别近期高引用论文和新兴方法
3. 发现趋势：LLM 压缩成为新热点
4. 注意到"最新"可能意味着方法创新，也可能意味着应用突破
5. 返回时按"基础方法 → 前沿进展 → 开放问题"组织
6. 注意到用户可能关心工程落地，补充实际应用案例
7. 如果用户进一步询问，基于已构建的知识图谱深入某个方向

**差异：**前者是信息检索，后者是研究助手。

---

## 六、何时使用流程，何时赋予自由度

并非所有场景都适合认知闭环设计。选择的关键在于任务的性质。

### 适合流程化的场景：

- **任务高度结构化**：报销审批、数据录入、标准化检查
- **正确性要求绝对严格**：金融交易、医疗处方、法律文书
- **环境高度可预测**：固定格式的文档处理、重复性操作
- **效率优先于灵活性**：大规模重复任务、实时响应要求

在这些场景中，流程化设计能保证：
- 零错误率
- 可审计性
- 高吞吐量
- 行为可预测

### 适合认知闭环的场景：

- **环境动态变化**：故障排查、安全应对、实时策略调整
- **目标模糊或多样**：创意设计、研究探索、开放式问题解决
- **需要长期适应**：个性化推荐、持续学习、用户行为预测
- **价值在于质量而非速度**：战略分析、复杂决策、深度研究

在这些场景中，认知闭环设计能实现：
- 适应性
- 创造性
- 持续改进
- 处理边界情况

### 混合策略：最实用的方案

最有效的 Agent 往往采用混合设计：
- 用流程保证基线能力和安全底线
- 用认知闭环处理例外和复杂情况
- 让 Agent 在安全范围内自由探索

**示例：智能客服**

```python
class HybridCustomerServiceAgent:
    def handle_query(self, query, context):
        # 第一层：快速分类
        category = self.quick_classify(query)
        
        # 标准问题：流程化快速响应
        if category in STANDARD_QUESTIONS:
            return self.standard_flow(category, query)
        
        # 复杂情况：切换到认知模式
        if category == "complex_complaint":
            return self.cognitive_mode(query, context)
        
        # 边界情况：探索性对话
        if category == "unknown":
            return self.exploratory_dialogue(query, context)
    
    def cognitive_mode(self, query, context):
        # 理解用户真实需求
        understanding = self.deep_understand(query, context)
        
        # 维持对话目标
        goal = self.identify_user_goal(understanding)
        
        # 自主选择解决路径
        solution = self.find_solution(goal, understanding)
        
        # 如果失败，调整策略
        if not solution.satisfactory:
            return self.adjust_and_retry(goal, solution.feedback)
        
        return solution
```

**关键原则：**
- 标准场景用流程保证效率
- 复杂场景用认知保证质量
- 动态切换，而非二选一

---

## 结语

Agent 的发展正在经历一次范式切换：从围绕可控性的工程系统，走向围绕能力增长的认知体。

Claude Code、AlphaStar、以及越来越多的实践案例展示了这种路线在现实任务中的有效性。当模型被放入一个可感知、可行动、可反馈的环境时，智能体的特征会自然涌现。

**这不是对工程的否定，而是重新定义了工程的目标：**

**旧目标：**通过工程手段让模型更可控。
**新目标：**通过工程手段让模型具备更强的内部能力， 用“赋能”取代“约束”。

差异在于，前者用结构限制模型，后者用结构赋能模型。

**我们需要停止用工程化手段扼杀 Agent。**

停止用预设流程替代决策能力。
停止用固定路径压缩探索空间。
停止用回退机制剥夺学习机会。

相反，我们应该：

为 Agent 构建持续的世界模型。
为 Agent 维持稳定的高层目标。
为 Agent 提供真实的环境反馈。
为 Agent 保留必要的自由度。

当我们设计下一个 Agent 时，值得问自己：

**我在构建一个执行器，还是一个认知体？**
**我在用工程控制它，还是在用工程赋能它？**

答案将决定这个 Agent 是一个精密的自动机，还是一个能够成长的智能体。

---

*© 2026 AI 科普系列 · 用简单的语言解释复杂的技术*
